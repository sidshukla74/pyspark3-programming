# -*- coding: utf-8 -*-
"""pysparkdataframe.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bffJd6g7-h2GMLKicF_L7xsez6Mt61WG
"""

columns = ["language","users_count"]
data = [("Java", "20000"), ("Python", "100000"), ("Scala", "3000")]

!pip install pyspark
import pyspark

import pyspark
from pyspark.sql import *

spark = SparkSession.builder.appName("sparkbyexaples.com").getOrCreate()
rdd = spark.sparkContext.parallelize(data)

df1 = rdd.toDF()
df1.printSchema()

columns =["language", "users_count"]
df1 = rdd.toDF(columns)
df1.printSchema()

df2 = spark.createDataFrame(rdd).toDF(*columns)

df2

df2 = spark.createDataFrame(data).toDF(*columns)

data

columns

rowdata = map(lambda x:Row(*x), data)
dffFromdata3 = spark.createDataFrame(rowdata, columns)

rowdata

dffFromdata3.show()

dffFromdata3

from pyspark.sql.types import StructType,StructField, StringType, IntegerType

data2 = [("James","","Smith","36636","M",3000),
    ("Michael","Rose","","40288","M",4000),
    ("Robert","","Williams","42114","M",4000),
    ("Maria","Anne","Jones","39192","F",4000),
    ("Jen","Mary","Brown","","F",-1)
  ]

schema = StructType([ \
    StructField("firstname",StringType(),True), \
    StructField("middlename",StringType(),True), \
    StructField("lastname",StringType(),True), \
    StructField("id", StringType(), True), \
    StructField("gender", StringType(), True), \
    StructField("salary", IntegerType(), True) \
  ])

df = spark.createDataFrame(data=data2,schema=schema)
df.printSchema()
df.show(truncate=False)

df2 = pd.read_csv('')

from google.colab import drive
df2 = spark.read.csv('/content/sample_data/california_housing_test.csv')

df2.show()

df3 = spark.read.json('/content/sample_data/anscombe.json')

df3

df3.show()

cols = ["sweden", "norway", "denmark"]
data = [("stokholm", "200000"), ("oslo", "100000"), ("denmark", "140000")]

cols

data

spark = SparkSession.builder.appName("scandanavian").getOrCreate()
rdd = spark.sparkContext.parallelize(data)

rdd

df1 = rdd.toDF()

df1

df1.show()

df1.printSchema()

df1 = rdd.toDF(["cities", "pop"])

df1.show()

df1.printSchema()

df2 = spark.createDataFrame(rdd).toDF(*columns)

df2

df2.show()

